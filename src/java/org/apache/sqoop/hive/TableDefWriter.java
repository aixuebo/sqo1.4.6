/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.sqoop.hive;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Map;
import java.util.Date;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Properties;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.sqoop.io.CodecMap;

import com.cloudera.sqoop.SqoopOptions;
import com.cloudera.sqoop.manager.ConnManager;

/**
 * Creates (Hive-specific) SQL DDL statements to create tables to hold data
 * we're importing from another source.
 * 创建一个hive的SQL的DDL建表语句等,创建table,目的是我们要导入的数据存储在该表中
 *
 * After we import the database into HDFS, we can inject it into Hive using
 * the CREATE TABLE and LOAD DATA INPATH statements generated by this object.
 * 当数据被导入到HDFS中后,我们要注入到hive中,使用CREATE TABLE and LOAD DATA INPATH 语法结构
 */
public class TableDefWriter {

  public static final Log LOG = LogFactory.getLog(
      TableDefWriter.class.getName());

  private SqoopOptions options;
  private ConnManager connManager;//数据库连接器,目的是获取table数据源的描述信息
  private Configuration configuration;
  private String inputTableName;//被加载到HDFS时候的输入源的table名字
  private String outputTableName;//导入到hive中的table名字
  private boolean commentsEnabled;//是否有备注DDL

  /**
   * Creates a new TableDefWriter to generate a Hive CREATE TABLE statement.
   * @param opts program-wide options
   * @param connMgr the connection manager used to describe the table. 数据库连接器,目的是获取table数据源的描述信息
   * @param inputTable the name of the table to load.
   * @param outputTable the name of the Hive table to create.
   * @param config the Hadoop configuration to use to connect to the dfs
   * @param withComments if true, then tables will be created with a
   *        timestamp comment.
   */
  public TableDefWriter(final SqoopOptions opts, final ConnManager connMgr,
      final String inputTable, final String outputTable,
      final Configuration config, final boolean withComments) {
    this.options = opts;
    this.connManager = connMgr;
    this.inputTableName = inputTable;
    this.outputTableName = outputTable;
    this.configuration = config;
    this.commentsEnabled = withComments;
  }

  private Map<String, Integer> externalColTypes;

  /**
   * Set the column type map to be used.设置每一个列的类型
   * (dependency injection for testing; not used in production.)测试的时候使用
   */
  public void setColumnTypes(Map<String, Integer> colTypes) {
    this.externalColTypes = colTypes;
    LOG.debug("Using test-controlled type map");
  }

  /**
   * Get the column names to import.
   * 获取要导入哪些数据库的字段
   */
  private String [] getColumnNames() {
    String [] colNames = options.getColumns();//获取列的name集合 columns参数 按照逗号拆分的集合
    if (null != colNames) {//说明设置了columns参数,说明只导入这几个字段
      return colNames; // user-specified column names.
    } else if (null != externalColTypes) {//用于测试
      // Test-injection column mapping. Extract the col names from this.
      ArrayList<String> keyList = new ArrayList<String>();
      for (String key : externalColTypes.keySet()) {
        keyList.add(key);
      }

      return keyList.toArray(new String[keyList.size()]);
    } else if (null != inputTableName) {//查询整个表所有的字段
      return connManager.getColumnNames(inputTableName);//获取输入源数据库的table的元数据信息
    } else {//查询sql中指代的字段
      return connManager.getColumnNamesForQuery(options.getSqlQuery());//获取输入源数据库的table的元数据信息  query参数的内容
    }
  }

  /**
   * @return the CREATE TABLE statement for the table to load into hive.
   * 创建hive的建表语句
   *
   */
  public String getCreateTableStmt() throws IOException {
    Map<String, Integer> columnTypes;//类的名字和类型
      //map-column-hive参数,用于表示设置每一个数据库属性对应的hive类型,格式${column}=${hive} 或者 在props中加载map.column.hive.${column}=${hive}的类型,返回值是${column}=${hive}
    Properties userMapping = options.getMapColumnHive();//hive映射属性

    if (externalColTypes != null) {//用于测试
      // Use pre-defined column types.
      columnTypes = externalColTypes;
    } else {
      // Get these from the database.
      if (null != inputTableName) {
        columnTypes = connManager.getColumnTypes(inputTableName);//获取sql表对应的列的类型
      } else {
        columnTypes = connManager.getColumnTypesForQuery(options.getSqlQuery());//通过sql查找对应的列的类型
      }
    }

    String [] colNames = getColumnNames();//获取要导入哪些数据库的字段
    StringBuilder sb = new StringBuilder();
    if (options.doFailIfHiveTableExists()) {//create-hive-table参数内容
      sb.append("CREATE TABLE `");
    } else {
      sb.append("CREATE TABLE IF NOT EXISTS `");
    }

    if(options.getHiveDatabaseName() != null) { //hive-database参数内容
      sb.append(options.getHiveDatabaseName()).append("`.`");
    }
    sb.append(outputTableName).append("` ( ");

    // Check that all explicitly mapped columns are present in result set
    //说明userMapping中的内容一定在colNames中存在,即colNames的内容多,要包含userMapping的内容,userMapping的内容是某些属性不存在hive映射的时候添加的内容,正常情况下不会设置该属性
    for(Object column : userMapping.keySet()) {
      boolean found = false;
      for(String c : colNames) {
        if (c.equals(column)) {
          found = true;
          break;
        }
      }

      if (!found) {
        throw new IllegalArgumentException("No column by the name " + column
                + "found while importing data");
      }
    }

    boolean first = true;
    String partitionKey = options.getHivePartitionKey();  //hive-partition-key参数内容
    //校验hive的分区不能在抓取的数据集合内
    for (String col : colNames) {
      if (col.equals(partitionKey)) {
        throw new IllegalArgumentException("Partition key " + col + " cannot "
            + "be a column to import.");
      }

      if (!first) {
        sb.append(", ");
      }

      first = false;

      Integer colType = columnTypes.get(col);
      String hiveColType = userMapping.getProperty(col);//该列对应的hive的类型
      if (hiveColType == null) {//正常情况下都是null
        hiveColType = connManager.toHiveType(inputTableName, col, colType);//根据字段的类型,即第三个参数映射属于哪个hive属性
      }
      if (null == hiveColType) {//说明该列在hive没有对应的类型
        throw new IOException("Hive does not support the SQL type for column "
            + col);
      }

      sb.append('`').append(col).append("` ").append(hiveColType);

      if (HiveTypes.isHiveTypeImprovised(colType)) {
        LOG.warn(
            "Column " + col + " had to be cast to a less precise type in Hive");
      }
    }

    sb.append(") ");

    if (commentsEnabled) {
      DateFormat dateFormat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
      String curDateStr = dateFormat.format(new Date());
      sb.append("COMMENT 'Imported by sqoop on " + curDateStr + "' ");
    }

    //添加分区
    if (partitionKey != null) {
      sb.append("PARTITIONED BY (")
        .append(partitionKey)
        .append(" STRING) ");
     }

    sb.append("ROW FORMAT DELIMITED FIELDS TERMINATED BY '");
    sb.append(getHiveOctalCharCode((int) options.getOutputFieldDelim())); //fields-terminated-by参数内容
    sb.append("' LINES TERMINATED BY '");
    sb.append(getHiveOctalCharCode((int) options.getOutputRecordDelim()));//参数lines-terminated-by内容
    String codec = options.getCompressionCodec(); //参数compression-codec的内容
    if (codec != null && (codec.equals(CodecMap.LZOP)
            || codec.equals(CodecMap.getCodecClassName(CodecMap.LZOP)))) {
      sb.append("' STORED AS INPUTFORMAT "
              + "'com.hadoop.mapred.DeprecatedLzoTextInputFormat'");
        sb.append(" OUTPUTFORMAT "
              + "'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'");
    } else {
      sb.append("' STORED AS TEXTFILE");
    }

    LOG.debug("Create statement: " + sb.toString());
    return sb.toString();
  }

  /**
   * @return the LOAD DATA statement to import the data in HDFS into hive.
   * 返回加载数据的hive语法
   * LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
   *
   * 例如:
   * 输出 LOAD DATA INPATH '$path' OVERWRITE INTO TABLE '$database'.'$table' PARTITION ($key='$value') 注意看代码好像目前就支持一个字段的分区
   */
  public String getLoadDataStmt() throws IOException {
    Path finalPath = getFinalPath();

    StringBuilder sb = new StringBuilder();
    sb.append("LOAD DATA INPATH '");
    sb.append(finalPath.toString() + "'");
    if (options.doOverwriteHiveTable()) {//hive-overwrite参数设置
      sb.append(" OVERWRITE");
    }
    sb.append(" INTO TABLE `");
    if(options.getHiveDatabaseName() != null) {//hive-database参数内容
      sb.append(options.getHiveDatabaseName()).append("`.`");
    }
    sb.append(outputTableName);
    sb.append('`');

    if (options.getHivePartitionKey() != null) {//hive-partition-key参数内容
      sb.append(" PARTITION (")
        .append(options.getHivePartitionKey()) //hive-partition-key参数内容
        .append("='").append(options.getHivePartitionValue()) //hive-partition-value参数内容
        .append("')");
    }

    LOG.debug("Load statement: " + sb.toString());
    return sb.toString();
  }

    //输出目录就是仓库+表名,或者是targetDir
  public Path getFinalPath() throws IOException {
    String warehouseDir = options.getWarehouseDir(); //warehouse-dir参数内容
    if (null == warehouseDir) {
      warehouseDir = "";
    } else if (!warehouseDir.endsWith(File.separator)) {
      warehouseDir = warehouseDir + File.separator;
    }

    // Final path is determined in the following order:
    // 1. Use target dir if the user specified.
    // 2. Use input table name.
    String tablePath = null;
    String targetDir = options.getTargetDir(); //target-dir参数内容
    if (null != targetDir) {
      tablePath = warehouseDir + targetDir;
    } else {
      tablePath = warehouseDir + inputTableName;
    }
    FileSystem fs = FileSystem.get(configuration);
    return new Path(tablePath).makeQualified(fs);
  }

  /**
   * Return a string identifying the character to use as a delimiter
   * in Hive, in octal representation.
   * Hive can specify delimiter characters in the form '\ooo' where
   * ooo is a three-digit octal number between 000 and 177. Values
   * may not be truncated ('\12' is wrong; '\012' is ok) nor may they
   * be zero-prefixed (e.g., '\0177' is wrong).
   *
   * @param charNum the character to use as a delimiter
   * @return a string of the form "\ooo" where ooo is an octal number
   * in [000, 177].
   * @throws IllegalArgumentException if charNum &gt; 0177.
   * 把int数字转换成16进制,用于设置hive的分隔符
   */
  public static String getHiveOctalCharCode(int charNum) {
    if (charNum > 0177) {
      throw new IllegalArgumentException(
          "Character " + charNum + " is an out-of-range delimiter");
    }

    return String.format("\\%03o", charNum);
  }

}

